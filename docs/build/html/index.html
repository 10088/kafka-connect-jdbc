

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Kafka Connect JDBC Sink &mdash; kafka-connect-jdbc-sink 0.1 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
    <link rel="top" title="kafka-connect-jdbc-sink 0.1 documentation" href="#"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> kafka-connect-jdbc-sink
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="simple">
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="#">kafka-connect-jdbc-sink</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="#">Docs</a> &raquo;</li>
      
    <li>Kafka Connect JDBC Sink</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/index.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="kafka-connect-jdbc-sink">
<h1>Kafka Connect JDBC Sink<a class="headerlink" href="#kafka-connect-jdbc-sink" title="Permalink to this headline">¶</a></h1>
<p>Kafka Connect JDBC Sink is a connector to write data from Kafka to a sink target that supports JDBC.</p>
<div class="toctree-wrapper compound">
<ul class="simple">
</ul>
</div>
<div class="section" id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li>Confluent 2.0.1</li>
<li>Java 1.7</li>
</ul>
</div>
<div class="section" id="sink-connector-quickstart">
<h2>Sink Connector QuickStart<a class="headerlink" href="#sink-connector-quickstart" title="Permalink to this headline">¶</a></h2>
<p>To see some of the features of the JDBC we will write data from a topic to a SQLLite database, in INSERT mode and use
the RETRY functionality to recover from errors on insert.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">You can use your favorite database instead of SQLite. Follow the same steps, but adjust the connection.url setting
for your database. Confluent Platform includes JDBC drivers for SQLite and PostgreSQL, but if you’re using a
different database you’ll also need to make sure the JDBC driver is available on the Kafka Connect process’s
CLASSPATH.</p>
</div>
<p>Start by creating a database (you’ll need to install SQLite if you haven’t already):</p>
<div class="highlight-bash"><div class="highlight"><pre>➜  ~ sqlite3 test.db
    SQLite version 3.8.10.2 2015-05-20 18:17:19
    Enter <span class="s2">&quot;.help&quot;</span> <span class="k">for</span> usage hints.
    sqlite&gt; create table orders <span class="o">(</span>id int, created varchar<span class="o">(</span>150<span class="o">)</span>, product text, quantity int, price float, PRIMARY KEY <span class="o">(</span>id<span class="o">))</span>
    sqlite&gt;
</pre></div>
</div>
<p>Now we create a configuration file that will load data from this database. This file is included with the connector in
<code class="docutils literal"><span class="pre">etc/kafka-connect-jdbc/quickstart-sqlite-sink.properties</span></code> and contains the following settings:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">name</span><span class="o">=</span>jdbc-datamountaineer-1
connector.class<span class="o">=</span>com.datamountaineer.streamreactor.connect.jdbc.sink.JdbcSinkConnector
tasks.max<span class="o">=</span>1
<span class="nv">topics</span><span class="o">=</span>orders
connect.jdbc.connection.uri<span class="o">=</span>jdbc:sqlite:test.db
connect.jdbc.connection.user<span class="o">=</span>
connect.jdbc.connection.password<span class="o">=</span>
connect.jdbc.sink.error.policy<span class="o">=</span>RETRY
connect.jdbc.sink.batching.enabled<span class="o">=</span><span class="nb">true</span>
connect.jdbc.sink.export.mappings<span class="o">={</span>orders:orders<span class="p">;</span>qty-&gt;quantity,product-&gt;,price-&gt;<span class="o">}</span>
connect.jdbc.sink.mode<span class="o">=</span>INSERT
</pre></div>
</div>
<p>This configuration defines source topic <code class="docutils literal"><span class="pre">topics</span></code>, the connection to target database  <code class="docutils literal"><span class="pre">connect.jdbc.connection.uri</span></code>,
the error policy <code class="docutils literal"><span class="pre">connect.jdbc.sink.error.policy</span></code>, the mappings of topics/fields <code class="docutils literal"><span class="pre">connect.jdbc.sink.export.mappings</span></code>
and the sink write mode <code class="docutils literal"><span class="pre">connect.jdbc.sink.mode</span></code>.</p>
<p>Now, run the connector as a standalone Kafka Connect worker in another terminal (this assumes Avro settings and that
Kafka and the Schema Registry are running locally on the default ports):</p>
<div class="highlight-bash"><div class="highlight"><pre>➜ /bin/connect-standalone etc/schema-registry/connect-avro-standalone.properties
etc/kafka-connect-jdbc/quickstart-sqlite-sink.properties
</pre></div>
</div>
<p>Check the sink is up with no errors.</p>
<div class="highlight-bash"><div class="highlight"><pre>    ____        __        __  ___                  __        _
   / __ <span class="se">\_</span>___ _/ /_____ _/  <span class="p">|</span>/  /___  __  ______  / /_____ _<span class="o">(</span>_<span class="o">)</span>___  ___  ___  _____
  / / / / __ <span class="sb">`</span>/ __/ __ <span class="sb">`</span>/ /<span class="p">|</span>_/ / __ <span class="se">\/</span> / / / __ <span class="se">\/</span> __/ __ <span class="sb">`</span>/ / __ <span class="se">\/</span> _ <span class="se">\/</span> _ <span class="se">\/</span> ___/
 / /_/ / /_/ / /_/ /_/ / /  / / /_/ / /_/ / / / / /_/ /_/ / / / / /  __/  __/ /
/_____/<span class="se">\_</span>_,_/<span class="se">\_</span>_/<span class="se">\_</span>_,_/_/  /_/<span class="se">\_</span>___/<span class="se">\_</span>_,_/_/ /_/<span class="se">\_</span>_/<span class="se">\_</span>_,_/_/_/ /_/<span class="se">\_</span>__/<span class="se">\_</span>__/_/
       ______  ____  ______   _____ _       __
      / / __ <span class="se">\/</span> __ <span class="o">)</span>/ ____/  / ___/<span class="o">(</span>_<span class="o">)</span>___  / /__  by Stefan Bocutiu
 __  / / / / / __  / /       <span class="se">\_</span>_ <span class="se">\/</span> / __ <span class="se">\/</span> //_/
/ /_/ / /_/ / /_/ / /___    ___/ / / / / / ,&lt;
<span class="se">\_</span>___/_____/_____/<span class="se">\_</span>___/   /____/_/_/ /_/_/<span class="p">|</span>_<span class="p">|</span>
 <span class="o">(</span>com.datamountaineer.streamreactor.connect.jdbc.sink.JdbcSinkTask:56<span class="o">)</span>
  INFO JdbcSinkConfig values:
    connect.jdbc.connection.user <span class="o">=</span>
    connect.jdbc.sink.mode <span class="o">=</span> INSERT
    connect.jdbc.sink.batching.enabled <span class="o">=</span> <span class="nb">true</span>
<span class="nb">    </span>connect.jdbc.sink.error.policy <span class="o">=</span> RETRY
    connect.jdbc.sink.max.retries <span class="o">=</span> 10
    connect.jdbc.connection.password <span class="o">=</span> <span class="o">[</span>hidden<span class="o">]</span>
    connect.jdbc.connection.uri <span class="o">=</span> jdbc:sqlite:/Users/andrew/test.db
    connect.jdbc.sink.export.mappings <span class="o">=</span> <span class="o">{</span>orders:orders<span class="p">;</span>qty-&gt;quantity,product-&gt;,price-&gt;<span class="o">}</span>
    connect.jdbc.sink.retry.interval <span class="o">=</span> 60000
</pre></div>
</div>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">We try to catch all configuration errors at start and fail fast. Check the sink is up and not throwing configuration
errors.</p>
</div>
<p>Next we need to add data to the orders topic we asked the sink to drain. Start the avro console producer:</p>
<div class="highlight-bash"><div class="highlight"><pre>➜ bin/kafka-avro-console-producer <span class="se">\</span>
 --broker-list localhost:9092 --topic orders <span class="se">\</span>
 --property value.schema<span class="o">=</span><span class="s1">&#39;{&quot;type&quot;:&quot;record&quot;,&quot;name&quot;:&quot;myrecord&quot;,&quot;fields&quot;:[{&quot;name&quot;:&quot;id&quot;,&quot;type&quot;:&quot;int&quot;},</span>
<span class="s1"> {&quot;name&quot;:&quot;product&quot;, &quot;type&quot;: &quot;string&quot;}, {&quot;name&quot;:&quot;qty&quot;, &quot;type&quot;: &quot;int&quot;}, {&quot;name&quot;:&quot;price&quot;, &quot;type&quot;: &quot;float&quot;}]}&#39;</span>
</pre></div>
</div>
<p>The producer console is now waiting for input. Copy and paste the following into the terminal:</p>
<div class="highlight-bash"><div class="highlight"><pre>➜ <span class="o">{</span><span class="s2">&quot;id&quot;</span>: 999, <span class="s2">&quot;product&quot;</span>: <span class="s2">&quot;foo&quot;</span>, <span class="s2">&quot;qty&quot;</span>: 100, <span class="s2">&quot;price&quot;</span>: 50<span class="o">}</span>
</pre></div>
</div>
<p>In our mappings we wanted <code class="docutils literal"><span class="pre">qty</span></code> to go to <code class="docutils literal"><span class="pre">quantity</span></code> in our table. Back in the logs on the sink you should see this:</p>
<div class="highlight-bash"><div class="highlight"><pre>INFO org.apache.kafka.connect.runtime.WorkerSinkTask@7fcca7e Committing offsets
INFO Received <span class="m">1</span> records. First entry topic:orders  partition:0 offset:0. Writing them to the database...
INFO Finished writing <span class="m">1</span> records to the database.
</pre></div>
</div>
<p>Check Sqlite:</p>
<div class="highlight-bash"><div class="highlight"><pre>sqlite&gt; <span class="k">select</span> * from orders<span class="p">;</span>
        999<span class="o">||</span>foo<span class="p">|</span>100<span class="p">|</span>50.0
</pre></div>
</div>
<p>Since we have set our error policy to RETRY we can test to see what happens if a second record is inserted with the same
primary key. Back at the producer console insert the same record again which will cause a primary key violation:</p>
<div class="highlight-bash"><div class="highlight"><pre>➜ <span class="o">{</span><span class="s2">&quot;id&quot;</span>: 999, <span class="s2">&quot;product&quot;</span>: <span class="s2">&quot;foo&quot;</span>, <span class="s2">&quot;qty&quot;</span>: 100, <span class="s2">&quot;price&quot;</span>: 50<span class="o">}</span>
</pre></div>
</div>
<p>You should now see a primary key constraint violation and the sink pausing and retrying:</p>
<div class="highlight-bash"><div class="highlight"><pre>ERROR An error has occurred inserting data starting at topic: orders offset: <span class="m">1</span> partition: 0
WARN Error policy <span class="nb">set </span>to RETRY. The following events will be replayed. Remaining attempts 9
WARN Going to retry inserting data starting at topic: orders offset: <span class="m">1</span> partition: 0

ERROR RetriableException from SinkTask jdbc-datamountaineer-1-0:
org.apache.kafka.connect.errors.RetriableException: java.sql.SQLException: UNIQUE constraint failed: orders.id
</pre></div>
</div>
<p>No lets fix this and have the sink recover. Connect to Sqlite again and delete the row:</p>
<div class="highlight-bash"><div class="highlight"><pre>sqlite&gt; delete from orders<span class="p">;</span>
</pre></div>
</div>
<p>If you check the logs of the sink you will see it recover and write the row:</p>
<div class="highlight-bash"><div class="highlight"><pre>INFO org.apache.kafka.connect.runtime.WorkerSinkTask@7fcca7e Committing offsets
INFO Received <span class="m">1</span> records. First entry topic:orders  partition:0 offset:1. Writing them to the database...
INFO Recovered from exception <span class="s2">&quot;UNIQUE constraint failed: orders.id&quot;</span> at 2016-05-19 10:55:55.010Z. Continuing to process...
INFO Finished writing <span class="m">1</span> records to the database.
INFO org.apache.kafka.connect.runtime.WorkerSinkTask@7fcca7e Committing offsets
</pre></div>
</div>
<p>Check Sqlite:</p>
<div class="highlight-bash"><div class="highlight"><pre>sqlite&gt; <span class="k">select</span> * from orders<span class="p">;</span>
        999<span class="o">||</span>foo<span class="p">|</span>100<span class="p">|</span>50.0
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The RETRY error handling is intended to allow operators to fix issues on databases without having to shutdown the
connectors, for example, a DBA could be alerted and fix an issue without having to know about operating a connector.</p>
</div>
</div>
<div class="section" id="features">
<h2>Features<a class="headerlink" href="#features" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li>Error Polices.</li>
<li>Write modes.</li>
<li>Topic to table mappings.</li>
<li>Field Selection.</li>
<li>Auto create tables.</li>
<li>Auto evolve tables.</li>
</ol>
<div class="section" id="error-polices">
<h3>Error Polices<a class="headerlink" href="#error-polices" title="Permalink to this headline">¶</a></h3>
<p>The sink has three error policies that determine how failed writes to the target database are handled.</p>
<p><strong>Throw</strong>.</p>
<p>Any error on write to the target database will be propagated up and processing is stopped. This is the default
behaviour.</p>
<p><strong>Noop</strong>.</p>
<p>Any error on write to the target database is ignored and processing continues.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This can lead to missed errors if you don&#8217;t have adequate monitoring. Data is not lost as it&#8217;s still in Kafka
subject to Kafka&#8217;s retention policy.</p>
</div>
<p><strong>Retry</strong>.</p>
<p>Any error on write to the target database causes the RetryIterable exception to be thrown. This causes the
Kafka connect framework to pause and replay the message. Offsets are not committed. For example, if the table is offline,
by accident or accidentally modified or even having a DDL statement applied cause a write failure, the message can be
replayed.</p>
<p>The error policies effect the behaviour of the schema evolution characteristics of the sink. See the schema evolution
section for more information.</p>
</div>
<div class="section" id="write-modes">
<h3>Write Modes<a class="headerlink" href="#write-modes" title="Permalink to this headline">¶</a></h3>
<p>The sink supports both <strong>insert</strong> and <strong>upsert</strong> modes.</p>
<p><strong>Insert</strong></p>
<p>In this mode the sink prepares insert statements to execute either in batch transactions or individually, dependent on
the <code class="docutils literal"><span class="pre">connect.jdbc.sink.batching.enabled</span></code> setting. Typically you would use this in append only tables such as ledgers.
Combined with the error policy setting, <code class="docutils literal"><span class="pre">connect.jdbc.sink.error.policy</span></code>, this allows for idempotent writes. For
example, sent to NOOP, violations of primary keys would be rejected by the database and sink would log the error and
continue processing but you miss real errors.</p>
<p><strong>Update</strong></p>
<p>In this mode the sink prepares upsert statements, the exact syntax is dependent on the target database.
The SQL dialect is obtained from the connection URI. When the sink tries to write, it executes the appropriate upsert
statement. For example, with MySQL it will use the
<a class="reference external" href="http://dev.mysql.com/doc/refman/5.7/en/insert-on-duplicate.html">ON DUPLICATE KEY</a> to apply an update if a primary key
constraint is violated. If the update fails the sink fails back to the error policy.</p>
<p>The following dialects and upsert statements are supported:</p>
<ol class="arabic simple">
<li>MySQL - <a class="reference external" href="http://dev.mysql.com/doc/refman/5.7/en/insert-on-duplicate.html">ON DUPLICATE KEY</a></li>
<li>ORACLE - <a class="reference external" href="https://docs.oracle.com/cd/B28359_01/server.111/b28286/statements_9016.htm">MERGE</a>.
This requires knowledge for the primary keys to build the merge statement. The database metadata is queried.</li>
<li>MSSQL - <a class="reference external" href="https://msdn.microsoft.com/en-us/library/bb510625.aspx">MERGE</a>.
This requires knowledge for the primary keys to build the merge statement. The database metadata is queried.
This requires knowledge for the primary keys to build the merge statement. The database metadata is queried.
to retrieve this.</li>
<li>PostgreSQL - <em>9.5 and above.</em> <a class="reference external" href="http://www.postgresql.org/docs/9.5/static/sql-insert.html">ON CONFLICT</a>.
This requires knowledge for the primary keys to build the merge statement. The database metadata is queried.</li>
</ol>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Postgre UPSERT is only supported on versions 9.5 and above.</p>
</div>
</div>
<div class="section" id="topic-routing">
<h3>Topic Routing<a class="headerlink" href="#topic-routing" title="Permalink to this headline">¶</a></h3>
<p>The sink supports topic routing that allows mapping the messages from topics to a specific table. For example map
a topic called &#8220;bloomberg_prices&#8221; to a table called &#8220;prices&#8221;. This mapping is set in the
<code class="docutils literal"><span class="pre">connect.jdbc.sink.export.mappings</span></code> option.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p class="last">Explicit mapping of topics to tables is required. If not present the sink will not start and fail validation checks.</p>
</div>
</div>
<div class="section" id="field-selection">
<h3>Field Selection<a class="headerlink" href="#field-selection" title="Permalink to this headline">¶</a></h3>
<p>The sink supports selecting fields from the source topic or selecting all fields and mapping of these fields to columns
in the target table. For example, map a field called &#8220;qty&#8221;  in a topic to a column called &#8220;quantity&#8221; in the target
table.</p>
<p>All fields can be selected by using &#8220;*&#8221; in the field part of <code class="docutils literal"><span class="pre">connect.jdbc.sink.export.mappings</span></code>.</p>
<p>Leaving the column name empty means trying to map to a column in the target table with the same name as the field in the
source topic.</p>
<div class="admonition tip">
<p class="first admonition-title">Tip</p>
<p>Check your mappings!</p>
<p>All database column names are checked at startup if mappings are provided. If they do not exist in the target table the
configuration will not pass validation checks and refuse to start. If <code class="docutils literal"><span class="pre">connect.jdbc.sink.auto.create</span></code> is enabled
this does not apply since the table will be created based on the first message received for the topic.</p>
<p class="last">If no mappings are supplied the checks happen when the first message is received and processed for the source topics.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p>Field selection disables evolving the target table if the upstream schema in the Kafka topic changes. By specifying
field mappings it is assumed the user is not interested in new upstream fields. For example they may be tapping into a
pipeline for a Kafka stream job and not be intended as the final recipient of the stream.</p>
<p class="last">If a upstream field is removed and the topic is not following the Schema Registry&#8217;s evolution rules, i.e. not
full or backwards compatible, any errors will default to the error policy. If schema evolution rules have been followed,
the missing field will return the default value set in the schema. A warning will be logged if the schema version
changes.</p>
</div>
</div>
<div class="section" id="auto-create-tables">
<h3>Auto Create Tables<a class="headerlink" href="#auto-create-tables" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
<div class="section" id="auto-evolve-tables">
<h3>Auto Evolve Tables<a class="headerlink" href="#auto-evolve-tables" title="Permalink to this headline">¶</a></h3>
<p>TODO</p>
</div>
</div>
<div class="section" id="configuration">
<h2>Configuration<a class="headerlink" href="#configuration" title="Permalink to this headline">¶</a></h2>
<p>The JDBC connector gives you quite a bit of flexibility in the databases you can export data to and how that data is
exported. This section first describes how to access databases whose drivers are not included with Confluent Platform,
then gives a few example configuration files that cover common scenarios, then provides an exhaustive description of the
available configuration options.</p>
<div class="section" id="jdbc-drivers">
<h3>JDBC Drivers<a class="headerlink" href="#jdbc-drivers" title="Permalink to this headline">¶</a></h3>
<p>The JDBC connector implements the data copying functionality on the generic JDBC APIs, but relies on JDBC drivers to
handle the database-specific implementation of those APIs. Confluent Platform ships with a few JDBC drivers, but if the
driver for your database is not included, you will need to make it available via the <code class="docutils literal"><span class="pre">CLASSPATH</span></code>.</p>
<p>One option is to install the JDBC driver jar alongside the connector. The packaged connector is installed in the
<code class="docutils literal"><span class="pre">share/java/kafka-connect-jdbc</span></code> directory, relative to the installation directory. If you have installed from Debian
or RPM packages, the connector will be installed in <code class="docutils literal"><span class="pre">/usr/share/java/kafka-connect-jdbc</span></code>. If you installed from zip or
tar files, the connector will be installed in the path given above under the directory where you unzipped the Confluent
Platform archive.</p>
<p>Alternatively, you can set the <code class="docutils literal"><span class="pre">CLASSPATH</span></code> variable before running. For example:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="nv">$ CLASSPATH</span><span class="o">=</span>/usr/local/firebird/* ./bin/copycat-distributed ./config/copycat-distributed.properties
</pre></div>
</div>
<p>would add the JDBC driver for the Firebird database, located in <code class="docutils literal"><span class="pre">/usr/local/firebird</span></code>, and allow you to use JDBC
connection URLs like <code class="docutils literal"><span class="pre">jdbc:firebirdsql:localhost/3050:/var/lib/firebird/example.db</span></code>.</p>
</div>
<div class="section" id="jdbc-sink-connector-configuration-options">
<h3>JDBC Sink Connector Configuration Options<a class="headerlink" href="#jdbc-sink-connector-configuration-options" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal"><span class="pre">connect.jdbc.connection.uri</span></code></p>
<p>Specifies the JDBC database connection URI.</p>
<ul class="simple">
<li>Type: string</li>
<li>Importance: high</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.connection.user</span></code></p>
<p>Specifies the JDBC connection user.</p>
<ul class="simple">
<li>Type: string</li>
<li>Importance: high</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.connection.password</span></code></p>
<p>Specifies the JDBC connection password.</p>
<ul class="simple">
<li>Type: password (shows <code class="docutils literal"><span class="pre">[hidden]</span></code> in logs)</li>
<li>Importance: high</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.batching.enabled</span></code></p>
<p>Specifies if a given sequence of SinkRecords are batched in a transaction or not. If <code class="docutils literal"><span class="pre">true</span></code> records delivered to
the task in each <cite>put</cite> are batched in one transaction. Otherwise each record is inserted in its own transaction.</p>
<ul class="simple">
<li>Type: boolean</li>
<li>Importance: high</li>
<li>Default : true</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.error.policy</span></code></p>
<p>Specifies the action to be taken if an error occurs while inserting the data.</p>
<p>There are three available options, <strong>noop</strong>, the error is swallowed, <strong>throw</strong>, the error is allowed to propagate and retry.
For <strong>retry</strong> the Kafka message is redelivered up to a maximum number of times specified by the <code class="docutils literal"><span class="pre">connect.jdbc.sink.max.retries</span></code>
option. The <code class="docutils literal"><span class="pre">connect.jdbc.sink.retry.interval</span></code> option specifies the interval between retries.</p>
<p>The errors will be logged automatically.</p>
<ul class="simple">
<li>Type: string</li>
<li>Importance: high</li>
<li>Default: <code class="docutils literal"><span class="pre">throw</span></code></li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.max.retries</span></code></p>
<p>The maximum number of times a message is retried. Only valid when the <code class="docutils literal"><span class="pre">connect.jdbc.sink.error.policy</span></code> is set to <code class="docutils literal"><span class="pre">retry</span></code>.</p>
<ul class="simple">
<li>Type: string</li>
<li>Importance: high</li>
<li>Default: 10</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.retry.interval</span></code></p>
<p>The interval, in milliseconds between retries if the sink is using <code class="docutils literal"><span class="pre">connect.jdbc.sink.error.policy</span></code> set to <strong>RETRY</strong>.</p>
<ul class="simple">
<li>Type: int</li>
<li>Importance: medium</li>
<li>Default : 60000 (1 minute)</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.mode</span></code></p>
<p>Specifies how the data should be landed into the RDBMS. Two options are supported: <strong>insert</strong> <strong>upsert</strong>.</p>
<ul class="simple">
<li>Type: string</li>
<li>Importance: high</li>
<li>Default: insert</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.export.mappings</span></code></p>
<p>Specifies to the mappings of topic to table. Additionally which fields to select from the source topic and their mappings
to columns in the target table. Multiple mappings can be set comma separated wrapped in {}. Before <code class="docutils literal"><span class="pre">;</span></code> is topic
to table mappings, after the field mappings.</p>
<p>Examples:</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="o">{</span>TOPIC1:TABLE1<span class="p">;</span>field1-&gt;col1,field5-&gt;col5,field7-&gt;col10<span class="o">}</span>
<span class="o">{</span>TOPIC2:TABLE2<span class="p">;</span>field1-&gt;,field2-&gt;<span class="o">}</span>
<span class="o">{</span>TOPIC3:TABLE3<span class="p">;</span>*<span class="o">}</span>
</pre></div>
</div>
<ul class="simple">
<li>Type: string</li>
<li>Importance: high</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Explicit mapping of topics to tables is required. If not present the sink will not start and fail validation checks.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Specifying * for field mappings means select and try to map all fields in the topic to matching fields in the target
table. Leaving the column name empty means trying to map to a column in the target table with the same name as the
field in the source topic.</p>
</div>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.evolve.tables</span></code></p>
<p>Specifies if the sink is allowed to evolve target tables. If set and new fields are add to the upstream topic
the sink will attempt to create a corresponding additional column in the mapped database. If an explict field mapping has
been set in <code class="docutils literal"><span class="pre">connect.jdbc.sink.export.mappings</span></code> the sink will <strong>not</strong> evolve the table as fields have been choosen by the
operator. The format is a comma separated list of the topics. The sink is then allowed to evolve the mapped tables.
For example, <code class="docutils literal"><span class="pre">connect.jdbc.sink.evolve.tables=topic1,topic2</span></code></p>
<ul class="simple">
<li>Type : string</li>
<li>Importance : medium</li>
<li>Default : not set.</li>
</ul>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Field selection disables this feature.</p>
</div>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.auto.create.tables</span></code></p>
<p>Mapping to enable auto table creation of tables for the configured topics. The tables are created based of the latest
schema for the topic. The format is {table:column list}, for example {topicA:a1,a2},{topicB:b1,b2},{topicC:}. The column
list is a comma separated list of fields to take from the topic to form the primary key. If left blank, as for topicC,
the sink will add a primary key column to the table called <code class="docutils literal"><span class="pre">__connect_auto_id</span></code>. This key will be filled with a
concatenated string if the form of topicname:partition:offset, e.g. topicC:1:99.</p>
<ul class="simple">
<li>Type : string</li>
<li>Importance: medium</li>
<li>Default : false</li>
</ul>
<p><code class="docutils literal"><span class="pre">connect.jdbc.sink.pk.col.name</span></code></p>
<p>The column name to create in a target table if <code class="docutils literal"><span class="pre">connect.jdbc.sink.auto.create.tables</span></code> is enabled and not fields set as
the primary key.</p>
<ul class="simple">
<li>Type : string</li>
<li>Importance : medium</li>
<li>Default : __connect_auto_id</li>
</ul>
</div>
<div class="section" id="example-configurations">
<h3>Example Configurations<a class="headerlink" href="#example-configurations" title="Permalink to this headline">¶</a></h3>
<p>The below example gives a typical example, specifying the connection details, error policy and if batching is enabled.
The most complicated option is the <code class="docutils literal"><span class="pre">connect.jdbc.sink.export.map</span></code>. This example has three mappings.</p>
<div class="highlight-bash"><div class="highlight"><pre><span class="c">#Name for the sink connector, must be unique in the cluster</span>
<span class="nv">name</span><span class="o">=</span>jdbc-datamountaineer-1
<span class="c">#Name of the Connector class</span>
connector.class<span class="o">=</span>com.datamountaineer.streamreactor.connect.jdbc.sink.JdbcSinkConnector
<span class="c">#Maximum number of tasks the Connector can start</span>
tasks.max<span class="o">=</span>5
<span class="c">#Input topics (Required by Connect Framework)</span>
<span class="nv">topics</span><span class="o">=</span>orders,otc_trades,greeks,bloomberg_prices
<span class="c">#Target database connection URI, MUST INCLUDE DATABASE</span>
connect.jdbc.connection.uri<span class="o">=</span>jdbc:mariadb://mariadb.landoop.com:3306/jdbc_sink_03
<span class="c">#Target database username and password</span>
connect.jdbc.connection.user<span class="o">=</span>testjdbcsink
connect.jdbc.connection.password<span class="o">=</span>datamountaineers
<span class="c">#Location of the JDBC jar</span>
connect.jdbc.sink.driver.jar<span class="o">=</span>/home/datamountaineers/connect-jdbc/connect/connect-hdfs-to-jdbc-wip/mariadb-java-client-1.4.4.jar
<span class="c">#Name of the JDBC Driver class to load</span>
connect.jdbc.sink.driver.manager.class<span class="o">=</span>org.mariadb.jdbc.Driver
<span class="c">#Error policy to handle failures (default is ``throw``)</span>
connect.jdbc.sink.error.policy<span class="o">=</span>THROW
<span class="c">#Enable batching, all records the a task receives each time it&#39;s called are batched together in one transaction</span>
connect.jdbc.sink.batching.enabled<span class="o">=</span><span class="nb">true</span>
<span class="c">#The topic to table mappings</span>
connect.jdbc.sink.export.mappings<span class="o">={</span>orders:orders_table<span class="p">;</span>product-&gt;product,qty-&gt;quantity,price-&gt;<span class="o">}</span>,<span class="o">{</span>otc_trades:trades<span class="p">;</span>*<span class="o">}</span>,<span class="o">{</span>bloomberg_prices:prices<span class="p">;</span><span class="nb">source</span>-&gt;,lst_bid-&gt;<span class="o">}</span>
<span class="c">#write mode</span>
connect.jdbc.sink.mode<span class="o">=</span>UPSERT
</pre></div>
</div>
<p>For the first mapping <strong>{orders:orders_table;product-&gt;product,qty-&gt;quantity,price-&gt;}</strong> tells the sink the following:</p>
<ol class="arabic simple">
<li>Map the <em>orders</em> topic to a table called <em>orders_table</em>.</li>
<li>Select fields <em>product</em>, <em>qty</em> and <em>price</em>  from the topic.</li>
<li>Map a field called <em>product</em>  from the <em>orders</em>  topic to a column called <em>product</em> in the <em>orders_table</em>.</li>
<li>Map a field called <em>qty</em>  from the <em>orders</em>  topic to a column called <em>quantity</em> in the <em>orders_table</em>.</li>
<li>Map a field called <em>price</em>  from the <em>orders</em>  topic to a column called <em>price</em> in the <em>orders_tables</em>. Here the target column
is left blank to the field name is taken.</li>
</ol>
<p>For the second mapping <strong>{otc_trades:trades;*}</strong> tells the sink the following:</p>
<ol class="arabic simple">
<li>Map the <em>otc_trades</em> topic to a table called <em>trades</em>.</li>
<li>Select all fields from the topics message and map them against matching column names in the trades table.
<strong>The * indicates select all fields from the topic.</strong></li>
</ol>
<p>The final mapping <strong>{bloomberg_prices:prices;source-&gt;,lst_bid-&gt;}</strong> tells the sink the following:</p>
<ol class="arabic simple">
<li>Map the <em>bloomberg_prices</em> topic to a table called <em>prices</em>.</li>
<li>Select fields <em>source</em>  and <em>lst_bid</em>  from the topic.</li>
<li>Map a field called <em>source</em>  from the <em>bloomberg_prices</em>  topic to a column called <em>source</em>  in the <em>prices</em> table.</li>
<li>Map a field called <em>lst_bid</em>  from the <em>bloomberg_prices</em>  topic to a column called <em>lst_bid</em>  in the <em>prices</em> table.</li>
</ol>
</div>
</div>
<div class="section" id="schema-evolution">
<h2>Schema Evolution<a class="headerlink" href="#schema-evolution" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
<div class="section" id="deployment-guidelines">
<h2>Deployment Guidelines<a class="headerlink" href="#deployment-guidelines" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
<div class="section" id="troubleshooting">
<h2>TroubleShooting<a class="headerlink" href="#troubleshooting" title="Permalink to this headline">¶</a></h2>
<p>TODO</p>
</div>
</div>


           </div>
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2016, Andrew Stevenson.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>